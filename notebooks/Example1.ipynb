{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## This package helps one compare and deploy models in two steps:\n",
    "\n",
    "1. Compare models built on most of your data (we have to hold some rows out for checking the accuracy, *this is referred to as the test set*)\n",
    "2. Pick the best approach, build this model using all of your data, save the model, and deploy predictions on test data to SQL Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "We make a connection and load in data. In this example we will load from a simple csv file. Usually we load data directly from a SQL Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\michael.mastanduno\\\\PycharmProjects\\\\healthcareai-py')\n",
    "import healthcareai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'healthcareai'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8fa3627986ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mhealthcareai\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDevelopSupervisedModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m df = pd.read_csv('../healthcareai/tests/fixtures/HCPyDiabetesClinical.csv',\n\u001b[1;32m      5\u001b[0m                      na_values=['None'])\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'healthcareai'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from healthcareai import DevelopSupervisedModel\n",
    "import pandas as pd\n",
    "import time\n",
    "df = pd.read_csv('../healthcareai/tests/fixtures/HCPyDiabetesClinical.csv',\n",
    "                     na_values=['None'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's glance at the first few records we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientEncounterID</th>\n",
       "      <th>PatientID</th>\n",
       "      <th>SystolicBPNBR</th>\n",
       "      <th>LDLNBR</th>\n",
       "      <th>A1CNBR</th>\n",
       "      <th>GenderFLG</th>\n",
       "      <th>ThirtyDayReadmitFLG</th>\n",
       "      <th>InTestWindowFLG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>167.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10001</td>\n",
       "      <td>153.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10001</td>\n",
       "      <td>170.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10002</td>\n",
       "      <td>187.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10002</td>\n",
       "      <td>188.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatientEncounterID  PatientID  SystolicBPNBR  LDLNBR  A1CNBR GenderFLG  \\\n",
       "0                   1      10001          167.0   195.0     4.2         M   \n",
       "1                   2      10001          153.0   214.0     5.0         M   \n",
       "2                   3      10001          170.0   191.0     4.0         M   \n",
       "3                   4      10002          187.0   135.0     4.4         M   \n",
       "4                   5      10002          188.0   125.0     4.3         M   \n",
       "\n",
       "  ThirtyDayReadmitFLG InTestWindowFLG  \n",
       "0                   N               N  \n",
       "1                   N               N  \n",
       "2                   N               N  \n",
       "3                   N               N  \n",
       "4                   N               N  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK that looks good. What are our column data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatientEncounterID       int64\n",
       "PatientID                int64\n",
       "SystolicBPNBR          float64\n",
       "LDLNBR                 float64\n",
       "A1CNBR                 float64\n",
       "GenderFLG               object\n",
       "ThirtyDayReadmitFLG     object\n",
       "InTestWindowFLG         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good, but let's say we had to change an **int** to a **factor** column (which might happen if the factor column is 0,1,2, etc). Also, we'll change an **object (factor)** column to a **float**.\n",
    "\n",
    "This is how:\n",
    "\n",
    "*Please note that in this example we are changing an integer ID to a float ID, which doesn't make any sense practically, but is used to illustrate the process.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['GenderFLG'] = df['GenderFLG'].astype(object) # changing to factor\n",
    "df['PatientEncounterID'] = df['PatientEncounterID'].astype(float) # to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up the data, let's do some preprocessing, split the data into train and test sets, and store the results in an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(43) # <-- used to make results reproducible\n",
    "o = DevelopSupervisedModel(modeltype='classification',\n",
    "                           df=df,\n",
    "                           predictedcol='ThirtyDayReadmitFLG',\n",
    "                           graincol='',#OPTIONAL/ENCOURAGED\n",
    "                           impute=True,\n",
    "                           debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've arranged the data and done imputation, let's create a logistic model and see how accurate it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
      "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
      "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)\n",
      "Best hyper-parameters found after tuning:\n",
      "No hyper-parameter tuning was done.\n",
      "\n",
      "AUC Score: 0.858630952381 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "o.linear(cores=1,\n",
    "         debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, so an AUC above 0.8 is fairly predictive, so the linear model did fairly well. (You'll note the cell above also specifies model details.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we've already done well, let's see how well a random forest does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Best hyper-parameters found after tuning:\n",
      "No hyper-parameter tuning was done.\n",
      "\n",
      "AUC Score: 0.902529761905 \n",
      "\n",
      "Variable importance:\n",
      "1. OrganizationLevel (0.488065)\n",
      "2. VacationHours (0.239089)\n",
      "3. SickLeaveHours (0.210164)\n",
      "4. Gender.M (0.032773)\n",
      "5. MaritalStatus.S (0.029909)\n"
     ]
    }
   ],
   "source": [
    "o.random_forest(cores=1,\n",
    "               debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, so that's interesting--random forest does even better with an AUC of 0.91. This means we'll choose to use the random forest model for nightly predictions. Random forest also gives us some guidance as to which variables are most important. If you have features that contribute below 0.1 in the variable importance list, you can safely leave them out of the deploy step (see the next example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback? Questions?\n",
    "\n",
    "Reach out to Levi Thatcher (levi.thatcher@healthcatalyst.com) if you have any questions!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}